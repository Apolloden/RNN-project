{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b11277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f'Device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beff39a",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ba0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    def __init__(self, file_path, seq_length, batch_size):\n",
    "        self.file_path = file_path\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_seq = None\n",
    "        self.data = None\n",
    "        self.X_data = None\n",
    "        self.Y_data = None\n",
    "        self.K = None\n",
    "        self.char_to_ind = None\n",
    "        self.ind_to_char = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Prepares all the data necessary to train a model\n",
    "        \"\"\"\n",
    "        fid = open(self.file_path, \"r\")\n",
    "        book_data = fid.read()\n",
    "        fid.close()\n",
    "        self.data = book_data\n",
    "        unique_chars = list(set(book_data))\n",
    "        K = len(unique_chars)\n",
    "        self.K = K\n",
    "        mapping_value = np.arange(K)\n",
    "        char_to_ind = dict(zip(unique_chars, mapping_value))\n",
    "        ind_to_char = dict(zip(mapping_value, unique_chars))\n",
    "        self.char_to_ind = char_to_ind\n",
    "        self.ind_to_char = ind_to_char\n",
    "\n",
    "    def get_one_hot_encoding(self, X_chars):\n",
    "        \"\"\"Encodes text as a one hot array\n",
    "\n",
    "        Args:\n",
    "            char_to_ind (dict): the mapping\n",
    "            X_chars (string): characters to encode\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: one-hot encoding\n",
    "        \"\"\"\n",
    "        seq_length = len(X_chars)\n",
    "        one_hot = np.zeros((self.K, seq_length))\n",
    "        for i, char in enumerate(X_chars):\n",
    "            ind = self.char_to_ind[char]\n",
    "            one_hot[ind, i] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def get_decoded_one_hot(self, Y):\n",
    "        \"\"\"Decodes one-hot array back to text\n",
    "\n",
    "        Args:\n",
    "            ind_to_char (dict): the mapping\n",
    "            Y (np.ndarray): one-hot encoding\n",
    "\n",
    "        Returns:\n",
    "            string: the decoded text\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        for t in range(Y.shape[1]):\n",
    "            char_max = np.argmax(Y[:, t])\n",
    "            text += self.ind_to_char[char_max]\n",
    "        return text\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Prepares the data as a tuple of inputs and targets \n",
    "        \"\"\"\n",
    "        encoded_data = self.get_one_hot_encoding(self.data)\n",
    "        num_sequences = (len(self.data)-1) // self.seq_length # discarding the tail\n",
    "        self.num_seq = num_sequences\n",
    "        sequences_X = []\n",
    "        sequences_Y = []\n",
    "        t = 0 # pointer in text\n",
    "        for seq in range(num_sequences):\n",
    "            inputs = encoded_data[:, t: t+self.seq_length]\n",
    "            targets = encoded_data[:, t+1: t+self.seq_length+1]\n",
    "            sequences_X.append(inputs)\n",
    "            sequences_Y.append(targets)\n",
    "            t += self.seq_length\n",
    "        self.X_data = np.concatenate(sequences_X, axis=1)\n",
    "        self.Y_data = np.concatenate(sequences_Y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1c5f0",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Creates a 1 layer RNN model, first there is a recurrent layer, \n",
    "    then a fully connected output layer. The activation function used is tanh.\n",
    "\n",
    "    Args:\n",
    "        nn (RNN): RNN model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, hprev):\n",
    "        \"\"\"Forward pass for the model\n",
    "        Args:\n",
    "            x (torch.Tensor): text (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "            hprev (torch.Tensor): previous hidden states (1, BATCH_SIZE, HIDDEN_STATES)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor, torch.Tensor: probabilities and updated hidden states\n",
    "        \"\"\"\n",
    "        out, hnext = self.rnn(x, hprev)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hnext\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        \"\"\"Initializes a new hidden layers of zeroes\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): batches\n",
    "            hidden_size (int): neurons in the hidden layer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: a new hidden layer (1, BATCH_SIZE, HIDDEN_SIZE)\n",
    "        \"\"\"\n",
    "        return torch.zeros(1, batch_size, hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd67bb",
   "metadata": {},
   "source": [
    "### Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc771969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, hprev, val_loader, device):\n",
    "    \"\"\"Uses a separate dataset to measure performace of the trained model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the trained model\n",
    "        hprev (torch.Tensor): previous hidden states\n",
    "        val_loader (dataloader): dataloader with the data\n",
    "        device (device): device for tensors\n",
    "\n",
    "    Returns:\n",
    "        mean loss, a list of all losses\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    smooth_loss = None\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets_indices = torch.argmax(targets, dim=2) # Shape (BATCH_SIZE, SEQ_LENGTH)\n",
    "\n",
    "            outputs, hnext = model(inputs, hprev)\n",
    "            hprev = hnext\n",
    "            preds = outputs.permute(0,2,1)      # shape (BATCH_SIZE, K, SEQ_LENGTH)\n",
    "            loss  = criterion(preds, targets_indices)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss.item()\n",
    "            else: \n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss.item()\n",
    "            val_loss.append(smooth_loss)\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / len(val_loader), val_loss\n",
    "\n",
    "def train(model, train_params, train_loader, val_loader):\n",
    "    \"\"\"Trains the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Model): model to train\n",
    "        train_params (dict): specific parameters used for training\n",
    "        train_loader (dataloader): dataloader with training data\n",
    "        val_loader (dataloader): dataloader with validation data\n",
    "\n",
    "    Returns:\n",
    "        Training statistics and losses\n",
    "    \"\"\"\n",
    "    \n",
    "    num_epochs = train_params['num_epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    hidden_size = train_params['hidden_size']\n",
    "    learning_rate = train_params['learning_rate']\n",
    "    \n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    update_step = 0\n",
    "    best_update_step = 0\n",
    "    best_epoch = 0\n",
    "    smooth_loss = None\n",
    "    best_smooth_loss = float('inf')\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        hprev = model.init_hidden(batch_size, hidden_size)\n",
    "        loss = 0.0\n",
    "        \n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            targets_indices = torch.argmax(targets, dim=2) # Shape (BATCH_SIZE, SEQ_LENGTH)# WHY?\n",
    "            hprev = hprev.detach() # WHY?\n",
    "                        \n",
    "            # FORWARD PASS\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hnext = model.forward(inputs, hprev) # outputs: shape (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "            hprev = hnext # update hidden states\n",
    "            preds = outputs.permute(0,2,1) # shape (BATCH_SIZE, K, SEQ_LENGTH)\n",
    "            loss  = criterion(preds, targets_indices)\n",
    "            \n",
    "            # BACKWARD PASS\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters with ADAM\n",
    "            optimizer.step()\n",
    "                    \n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss.item()\n",
    "            else: \n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss.item()\n",
    "            train_loss.append(smooth_loss)\n",
    "            \n",
    "            if smooth_loss < best_smooth_loss:\n",
    "                best_smooth_loss = smooth_loss\n",
    "                best_epoch = epoch\n",
    "                best_update_step = update_step\n",
    "            update_step += 1\n",
    "            \n",
    "        # Validation loss\n",
    "        validation_loss_mean, val_losses = validation_loop(model, hprev, val_loader, device)\n",
    "        val_loss = val_loss + val_losses\n",
    "            \n",
    "    print(f'---Training complete---\\nBest model had smooth loss: {best_smooth_loss}, at epoch: {best_epoch}, update step: {best_update_step}')\n",
    "    return train_loss, val_loss, best_smooth_loss, best_epoch, best_update_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc83a8",
   "metadata": {},
   "source": [
    "### Hyperparameters Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(model, \n",
    "                          param_grid:dict, \n",
    "                          train_func,\n",
    "                          train_ds,\n",
    "                          val_ds,\n",
    "                          device,\n",
    "                          max_trials:int = None):\n",
    "    \"\"\"\"\n",
    "    Performs a ranodm or grid search over the hyperparameters\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        param_grid (dict): hyperparameters to search\n",
    "        train_func (function): function to train the model\n",
    "        train_ds (TensorDataset): training dataset\n",
    "        val_ds (TensorDataset): validation dataset\n",
    "        device (device): device for tensors\n",
    "        max_trials (int, optional): maximum number of trials. Defaults to None.\n",
    "    Returns:\n",
    "        all parameter combinations and print best combination with lowest validation loss\n",
    "    \"\"\" \n",
    " \n",
    "    results = []\n",
    "    keys = list(param_grid.keys())\n",
    "    all_combinations = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "    if max_trials:\n",
    "    \n",
    "        all_combinations = random.sample(all_combinations, min(max_trials,len(all_combinations)))\n",
    "        \n",
    "\n",
    "    for trial_id, values in enumerate(all_combinations):\n",
    "        # id from0\n",
    "        trial_config = dict(zip(keys, values))\n",
    "        \n",
    "        print(f\"\\n Trial {trial_id + 1}/{len(all_combinations)}: {trial_config}\")\n",
    "\n",
    "        # create model\n",
    "        #model_train = model(**trial_config).to(device)\n",
    "        model_train = model(input_size=DP.K,\n",
    "               hidden_size=trial_config[\"hidden_size\"],\n",
    "                num_layers=1,\n",
    "               output_size=DP.K).to(device)\n",
    "\n",
    "        # train model\n",
    "        batch_size = trial_config['batch_size']\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        train_losses ,val_loss, *_ = train_func(model_train, trial_config, train_loader, val_loader)\n",
    "\n",
    "\n",
    "        # record\n",
    "        trial_result = trial_config.copy()\n",
    "        trial_result['val_loss'] = round(sum(val_loss) / len(val_loss), 4)\n",
    "        results.append(trial_result)\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # plot train loss\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(train_losses)\n",
    "        plt.title(f\"Training Loss for Trial {trial_id + 1}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.savefig(f\"train_loss_trial_{trial_id + 1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # best result\n",
    "        best_result = min(results, key=lambda x: x['val_loss'])\n",
    "        print(f\"\\n Best Result so far: {best_result}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def ablation_experiments(model,\n",
    "                          param_name,\n",
    "                            param_values,\n",
    "                              fixed_config, \n",
    "                              train_func, \n",
    "                              train_ds,\n",
    "                                val_ds,\n",
    "                                  device):\n",
    "    \"\"\"\n",
    "    Performs ablation experiments by varying one parameter at a time\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        param_name (str): name of the hyperparameter to vary\n",
    "        param_values (list): values of the hyperparameter to test\n",
    "        fixed_config (dict): fixed hyperparameters\n",
    "        train_func (function): function to train the model\n",
    "        train_ds (TensorDataset): training dataset\n",
    "        val_ds (TensorDataset): validation dataset\n",
    "        device (device): device for tensors         \n",
    "        Returns:\n",
    "            results (list): list of results for each parameter value\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for val in param_values:\n",
    "        config = fixed_config.copy()\n",
    "        config[param_name] = val\n",
    "\n",
    "        model_train = model(\n",
    "            input_size=DP.K,\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            output_size=DP.K,\n",
    "            num_layers=1  # 固定为单层RNN\n",
    "        ).to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "        train_losses ,val_loss, *_ = train_func(model_train, config, train_loader, val_loader)\n",
    "\n",
    "        results.append({\n",
    "            \"param_value\": val,\n",
    "            \"val_loss\": round(sum(val_loss) / len(val_loss), 4)\n",
    "        })\n",
    "    print(results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee00673",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3362229",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "DP.load_data()\n",
    "DP.preprocess()\n",
    "\n",
    "X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "# 70% train, 15% val, 15% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "X_val_t   = torch.from_numpy(X_val).float()\n",
    "Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "X_test_t  = torch.from_numpy(X_test).float()\n",
    "Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "model = RNNModel(\n",
    "    input_size=DP.K,\n",
    "    hidden_size=train_params['hidden_size'],\n",
    "    num_layers=train_params['num_layers'],\n",
    "    output_size=DP.K,\n",
    ").to(device)\n",
    "\n",
    "train_loss, val_loss, best_smooth_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7fe26",
   "metadata": {},
   "source": [
    "### Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "loss_fig_name = f'loss_fig_loss_{best_smooth_loss}_epoch_{best_epoch}_update_step_{best_update_step}.png'\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Smooth Training Loss')\n",
    "plt.xlabel('update steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig(loss_fig_name, bbox_inches='tight', facecolor='none', pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss history\n",
    "val_loss_fig_name = f'val_loss_fig_loss_{best_smooth_loss}_epoch_{best_epoch}_update_step_{best_update_step}.png'\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(val_loss, label='Smooth Validation Loss', color='orange')\n",
    "plt.xlabel('update steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig(loss_fig_name, bbox_inches='tight', facecolor='none', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2001d",
   "metadata": {},
   "source": [
    "### RNN Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {       \n",
    "    \"hidden_size\": [128, 256, 512], \n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"num_epochs\": [10]    \n",
    "}\n",
    "\n",
    "results = hyperparameter_search(\n",
    "    model=RNNModel,\n",
    "    param_grid=param_grid,\n",
    "    train_func=train,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    device=device,\n",
    "    max_trials=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e71f46",
   "metadata": {},
   "source": [
    "### Ablation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59717246",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_config = {\n",
    "    \"hidden_size\": 256,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 10\n",
    "}\n",
    "hidden_size_results = ablation_experiments(\n",
    "    model=RNNModel,\n",
    "    param_name=\"learning_rate\",\n",
    "    param_values=[0.001, 0.1],\n",
    "    fixed_config=fixed_config,\n",
    "    train_func=train,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d0008",
   "metadata": {},
   "source": [
    "### Text synthesis and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a107588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_old(\n",
    "    model,\n",
    "    DP,\n",
    "    gen_length: int,\n",
    "    temperature: float = 0.5,\n",
    "    device: torch.device = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text from an unprompted LSTMmodel using temperature-controlled sampling.\n",
    "\n",
    "    Args:\n",
    "        model:       your trained LSTMmodel instance\n",
    "        DP:   instance of your DataPreprocessing class, with:\n",
    "                     - get_one_hot_encoding(str) -> np.ndarray of shape (K, 1)\n",
    "                     - ind_to_char: dict mapping indices back to chars\n",
    "                     - char_to_ind: dict mapping chars to indices\n",
    "        gen_length:  number of characters to generate\n",
    "        temperature: >0. Lower = more conservative, higher = more random\n",
    "        device:      torch.device (default: model's device)\n",
    "\n",
    "    Returns:\n",
    "        A generated string of length `gen_length`.\n",
    "    \"\"\"\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(\"Temperature must be > 0\")\n",
    "\n",
    "    model.eval()\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.to(device)\n",
    "\n",
    "    h = torch.zeros(model.num_layers, 1, model.hidden_size, device=device)\n",
    "\n",
    "    K = len(DP.char_to_ind)\n",
    "    idx = torch.randint(0, K, (1,), device=device).item()\n",
    "    last_char = DP.ind_to_char[idx]\n",
    "    generated = last_char\n",
    "\n",
    "    # 3) feed that char once to set up hidden state\n",
    "    oh = DP.get_one_hot_encoding(last_char)            # (K, 1)\n",
    "    x = torch.tensor(oh.T, dtype=torch.float, device=device).unsqueeze(0)  # (1, 1, K)\n",
    "    with torch.no_grad():\n",
    "        _, h = model.rnn(x, h)\n",
    "\n",
    "    # 4) generate the rest\n",
    "    for _ in range(gen_length - 1):\n",
    "        oh = DP.get_one_hot_encoding(last_char)\n",
    "        x = torch.tensor(oh.T, dtype=torch.float, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            out, h = model.rnn(x, h)       # out: (1, 1, H)\n",
    "            logits = model.fc(out[:, -1, :])          # (1, K)\n",
    "            logits = logits / temperature\n",
    "            probs  = F.softmax(logits, dim=-1)        # (1, K)\n",
    "\n",
    "        idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        next_char = DP.ind_to_char[idx]\n",
    "        generated += next_char\n",
    "        last_char = next_char\n",
    "\n",
    "    return generated\n",
    "\n",
    "def generate_text(model, hprev, DP, text_len, input_text, temperature=0.5):\n",
    "    \n",
    "    input_indicies = []\n",
    "    for char in input_text:\n",
    "        input_indicies.append(DP.get_one_hot_encoding(char))\n",
    "    input_indicies = np.concatenate(input_indicies, axis=1) # (K, SEQ_LENGTH)\n",
    "    input_indicies = input_indicies.T # (SEQ_LENGTH, K)\n",
    "    input_indicies = np.expand_dims(input_indicies, axis=0) # (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "    input_indicies = torch.tensor(input_indicies, dtype=torch.float, device=device)    \n",
    "    generated_text = input_text\n",
    "    \n",
    "    model.eval()\n",
    "    for char_index in range(text_len):\n",
    "        predictions, hnext = model(input_indicies, hprev) # (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "        hprev = hnext\n",
    "        predictions = predictions[0, 0, :] # (K)\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predictions = torch.softmax(predictions, dim=-1)\n",
    "        prediction_id = torch.multinomial(predictions, num_samples=1).item()\n",
    "        next_char = DP.ind_to_char[prediction_id]\n",
    "        generated_text += next_char\n",
    "        \n",
    "        # Update the next input char\n",
    "        input_indicies = torch.zeros(1, 1, DP.K, device=device)\n",
    "        input_indicies[0, 0, prediction_id] = 1.0\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51114dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprev = model.init_hidden(batch_size=1, hidden_size=train_params['hidden_size'])\n",
    "text = generate_text(model, hprev, DP, 1000, \"ROMEO: \", 0.5)\n",
    "print(text)\n",
    "hprev = model.init_hidden(batch_size=train_params['batch_size'], hidden_size=train_params['hidden_size'])\n",
    "\n",
    "mean_test_loss, test_losses = validation_loop(model, hprev, test_loader, device)\n",
    "print(f'\\nFinal mean test accuarcy = {mean_test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
