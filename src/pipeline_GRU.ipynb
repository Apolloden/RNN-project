{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b11277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "rng = np.random.default_rng()\n",
    "import copy\n",
    "import metrics as m\n",
    "import perplexityGRU as g\n",
    "from bert_score import score as bert_score_score\n",
    "import itertools\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f'Device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed=61):\n",
    "    random.seed(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.manual_seed(seed) \n",
    "        except AttributeError:\n",
    "            pass \n",
    "\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n",
    "set_global_seed(61)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beff39a",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ba0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    def __init__(self, file_path, seq_length, batch_size):\n",
    "        self.file_path = file_path\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_seq = None\n",
    "        self.data = None\n",
    "        self.X_data = None\n",
    "        self.Y_data = None\n",
    "        self.K = None\n",
    "        self.char_to_ind = None\n",
    "        self.ind_to_char = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Prepares all the data necessary to train a model\n",
    "        \"\"\"\n",
    "        fid = open(self.file_path, \"r\")\n",
    "        book_data = fid.read()\n",
    "        fid.close()\n",
    "        self.data = book_data\n",
    "        unique_chars = list(set(book_data))\n",
    "        K = len(unique_chars)\n",
    "        self.K = K\n",
    "        mapping_value = np.arange(K)\n",
    "        char_to_ind = dict(zip(unique_chars, mapping_value))\n",
    "        ind_to_char = dict(zip(mapping_value, unique_chars))\n",
    "        self.char_to_ind = char_to_ind\n",
    "        self.ind_to_char = ind_to_char\n",
    "\n",
    "    def get_one_hot_encoding(self, X_chars):\n",
    "        \"\"\"Encodes text as a one hot array\n",
    "\n",
    "        Args:\n",
    "            char_to_ind (dict): the mapping\n",
    "            X_chars (string): characters to encode\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: one-hot encoding\n",
    "        \"\"\"\n",
    "        seq_length = len(X_chars)\n",
    "        one_hot = np.zeros((self.K, seq_length))\n",
    "        for i, char in enumerate(X_chars):\n",
    "            ind = self.char_to_ind[char]\n",
    "            one_hot[ind, i] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def get_decoded_one_hot(self, Y):\n",
    "        \"\"\"Decodes one-hot array back to text\n",
    "\n",
    "        Args:\n",
    "            ind_to_char (dict): the mapping\n",
    "            Y (np.ndarray): one-hot encoding\n",
    "\n",
    "        Returns:\n",
    "            string: the decoded text\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        for t in range(Y.shape[1]):\n",
    "            char_max = np.argmax(Y[:, t])\n",
    "            text += self.ind_to_char[char_max]\n",
    "        return text\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Prepares the data as a tuple of inputs and targets \n",
    "        \"\"\"\n",
    "        encoded_data = self.get_one_hot_encoding(self.data)\n",
    "        num_sequences = (len(self.data)-1) // self.seq_length # discarding the tail\n",
    "        self.num_seq = num_sequences\n",
    "        sequences_X = []\n",
    "        sequences_Y = []\n",
    "        t = 0 # pointer in text\n",
    "        for seq in range(num_sequences):\n",
    "            inputs = encoded_data[:, t: t+self.seq_length]\n",
    "            targets = encoded_data[:, t+1: t+self.seq_length+1]\n",
    "            sequences_X.append(inputs)\n",
    "            sequences_Y.append(targets)\n",
    "            t += self.seq_length\n",
    "        self.X_data = np.concatenate(sequences_X, axis=1)\n",
    "        self.Y_data = np.concatenate(sequences_Y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1c5f0",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"Creates a GRU model, first there is a GRU layer, \n",
    "    then a fully connected output layer. The activation function used is tanh.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hprev):\n",
    "        \"\"\"Forward pass for the model\n",
    "        Args:\n",
    "            x (torch.Tensor): text (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "            hprev (torch.Tensor): previous hidden states (1, BATCH_SIZE, HIDDEN_STATES)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor, torch.Tensor: logits and updated hidden states\n",
    "        \"\"\"\n",
    "        out, hnext = self.gru(x, hprev)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hnext\n",
    "\n",
    "    def init_hidden(self, num_layers, batch_size, hidden_size):\n",
    "        \"\"\"Initializes a new hidden layers of zeroes\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): number of hidden layers\n",
    "            batch_size (int): batches\n",
    "            hidden_size (int): neurons in the hidden layer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: a new hidden layer (NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "        \"\"\"\n",
    "        return torch.zeros(num_layers, batch_size, hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd67bb",
   "metadata": {},
   "source": [
    "### Training and Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc771969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, hprev, val_loader, device):\n",
    "    \"\"\"Uses a separate dataset to measure performace of the trained model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the trained model\n",
    "        hprev (torch.Tensor): previous hidden states\n",
    "        val_loader (dataloader): dataloader with the data\n",
    "        device (device): device for tensors\n",
    "\n",
    "    Returns:\n",
    "        mean loss, a list of all losses\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    smooth_loss = None\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets_indices = torch.argmax(targets, dim=2) # Shape (BATCH_SIZE, SEQ_LENGTH)\n",
    "\n",
    "            outputs, hnext = model(inputs, hprev)\n",
    "            hprev = hnext\n",
    "            preds = outputs.permute(0,2,1)      # shape (BATCH_SIZE, K, SEQ_LENGTH)\n",
    "            loss  = criterion(preds, targets_indices)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss.item()\n",
    "            else: \n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss.item()\n",
    "            val_loss.append(smooth_loss)\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / len(val_loader), val_loss\n",
    "\n",
    "def train(model, train_params, train_loader, val_loader):\n",
    "    \"\"\"Trains the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Model): model to train\n",
    "        train_params (dict): specific parameters used for training\n",
    "        train_loader (dataloader): dataloader with training data\n",
    "        val_loader (dataloader): dataloader with validation data\n",
    "\n",
    "    Returns:\n",
    "        Training statistics and losses\n",
    "    \"\"\"\n",
    "    \n",
    "    num_epochs = train_params['num_epochs']\n",
    "    num_layers = train_params['num_layers']\n",
    "    batch_size = train_params['batch_size']\n",
    "    hidden_size = train_params['hidden_size']\n",
    "    learning_rate = train_params['learning_rate']\n",
    "    \n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    update_step = 0\n",
    "    best_update_step = 0\n",
    "    best_epoch = 0\n",
    "    smooth_loss = None\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_loss_epoch = []\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        hprev = model.init_hidden(num_layers, batch_size, hidden_size)\n",
    "        loss = 0.0\n",
    "        \n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            targets_indices = torch.argmax(targets, dim=2) # Shape (BATCH_SIZE, SEQ_LENGTH)# WHY?\n",
    "            hprev = hprev.detach() # WHY?\n",
    "                        \n",
    "            # FORWARD PASS\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hnext = model.forward(inputs, hprev) # outputs: shape (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "            hprev = hnext # update hidden states\n",
    "            preds = outputs.permute(0,2,1) # shape (BATCH_SIZE, K, SEQ_LENGTH)\n",
    "            loss  = criterion(preds, targets_indices)\n",
    "            \n",
    "            # BACKWARD PASS\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters with ADAM\n",
    "            optimizer.step()\n",
    "                    \n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss.item()\n",
    "            else: \n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss.item()\n",
    "            train_loss.append(smooth_loss)\n",
    "            \n",
    "            update_step += 1\n",
    "            \n",
    "        # Validation loss\n",
    "        validation_loss_mean, val_losses = validation_loop(model, hprev, val_loader, device)\n",
    "        val_loss = val_loss + val_losses\n",
    "        val_loss_epoch.append(validation_loss_mean)\n",
    "        \n",
    "         # Save best model\n",
    "        if validation_loss_mean < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = validation_loss_mean\n",
    "            best_update_step = update_step\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "    print(f'---Training complete---\\nBest model had validation smooth loss: {best_val_loss}, at epoch: {best_epoch}, best update step {best_update_step}')\n",
    "    return best_model, train_loss, val_loss, val_loss_epoch, best_val_loss, best_epoch, best_update_step\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a1108",
   "metadata": {},
   "source": [
    "### Hyperparameter search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed783ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(model, \n",
    "                          param_grid:dict, \n",
    "                          train_func,\n",
    "                          train_ds,\n",
    "                          val_ds,\n",
    "                          device,\n",
    "                          max_trials:int = None):\n",
    "    \n",
    "    \"\"\"Performs a hyperparameter search over the given parameter grid.\n",
    "    Args:\n",
    "        model: RNN\n",
    "        param_grid (dict): dictionary with hyperparameters and their values\n",
    "        train_func (function): function to train the model\n",
    "        train_ds (TensorDataset): training dataset\n",
    "        val_ds (TensorDataset): validation dataset\n",
    "        device (str): device to use for training\n",
    "        max_trials (int, optional): maximum number of trials to run. Defaults to None.\n",
    "    Returns:\n",
    "        list: list of dictionaries with the results of each trial\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    keys = list(param_grid.keys())\n",
    "    all_combinations = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "    if max_trials:\n",
    "        all_combinations = random.sample(all_combinations, min(max_trials,len(all_combinations)))\n",
    "\n",
    "    for trial_id, values in enumerate(all_combinations):\n",
    "        trial_config = dict(zip(keys, values))\n",
    "        \n",
    "        print(f\"\\n Trial {trial_id + 1}/{len(all_combinations)}: {trial_config}\")\n",
    "\n",
    "        # Create model\n",
    "        model_train = model(\n",
    "            input_size=DP.K,\n",
    "            hidden_size=trial_config[\"hidden_size\"],\n",
    "            num_layers=1, \n",
    "            output_size=DP.K\n",
    "        ).to(device)\n",
    "\n",
    "        # Add missing training parameters\n",
    "        train_params = {\n",
    "            'num_epochs': 5,  # Add default number of epochs\n",
    "            'batch_size': trial_config['batch_size'],\n",
    "            'hidden_size': trial_config['hidden_size'],\n",
    "            'learning_rate': trial_config['learning_rate']\n",
    "        }\n",
    "\n",
    "        # Setup data loaders\n",
    "        batch_size = train_params['batch_size']\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "        # Train model with complete parameters\n",
    "        train_losses, val_loss, *_ = train_func(model_train, train_params, train_loader, val_loader)\n",
    "\n",
    "        # Record results\n",
    "        trial_result = trial_config.copy()\n",
    "        trial_result['val_loss'] = round(sum(val_loss) / len(val_loss), 4)\n",
    "        results.append(trial_result)\n",
    "\n",
    "        # Plot training loss\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(train_losses)\n",
    "        plt.title(f\"Training Loss for Trial {trial_id + 1}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.savefig(f\"train_loss_trial_{trial_id + 1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Track best result\n",
    "        best_result = min(results, key=lambda x: x['val_loss'])\n",
    "        print(f\"\\n Best Result so far: {best_result}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee00673",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3362229",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes_illiad.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 50,\n",
    "    'hidden_size': 145,\n",
    "    'num_layers': 2,\n",
    "    'num_epochs': 1,\n",
    "    'learning_rate': 0.000845663544150917\n",
    "}\n",
    "\n",
    "DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "DP.load_data()\n",
    "DP.preprocess()\n",
    "print(len(DP.data))\n",
    "print(DP.K)\n",
    "\n",
    "X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "# 70% train, 15% val, 15% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "X_val_t   = torch.from_numpy(X_val).float()\n",
    "Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "X_test_t  = torch.from_numpy(X_test).float()\n",
    "Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "model = GRUModel(\n",
    "    input_size=DP.K,\n",
    "    hidden_size=train_params['hidden_size'],\n",
    "    num_layers=train_params['num_layers'],\n",
    "    output_size=DP.K,\n",
    ").to(device)\n",
    "\n",
    "best_model, train_loss, val_loss, val_loss_epoch, best_smooth_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79428550",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ffafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'num_epochs': 20,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "# Coarse random search for learning rate parameter\n",
    "batch_choices = [64]\n",
    "hidden_choices = [50, 150, 250, 500]\n",
    "layers_choices = [2]\n",
    "lr_min = -4\n",
    "lr_max = -2\n",
    "for i in range(10):\n",
    "    lr = lr_min + (lr_max - lr_min) * rng.random()\n",
    "    lr = 10**lr\n",
    "    train_params['learning_rate'] = lr\n",
    "    train_params['batch_size'] = random.choice(batch_choices)\n",
    "    train_params['hidden_size'] = random.choice(hidden_choices)\n",
    "    train_params['num_layers'] = random.choice(layers_choices)\n",
    "    \n",
    "    DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "    DP.load_data()\n",
    "    DP.preprocess()\n",
    "\n",
    "    X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "    Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "    X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "    Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "    # 70% train, 15% val, 15% test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "    X_val_t   = torch.from_numpy(X_val).float()\n",
    "    Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "    X_test_t  = torch.from_numpy(X_test).float()\n",
    "    Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "    val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "    test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    model = GRUModel(\n",
    "        input_size=DP.K,\n",
    "        hidden_size=train_params['hidden_size'],\n",
    "        num_layers=train_params['num_layers'],\n",
    "        output_size=DP.K,\n",
    "    ).to(device)\n",
    "    \n",
    "    best_model, train_loss, val_loss, val_loss_epoch, best_smooth_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)\n",
    "    print(f\"Model completed - lr={train_params['learning_rate']}, batch_size={train_params['batch_size']}, hidden_size={train_params['hidden_size']}, num_layers={train_params['num_layers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc82372",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "# Coarse random search for learning rate parameter\n",
    "batch_choices = [64]\n",
    "hidden_choices = [150]\n",
    "layers_choices = [2]\n",
    "best_coarse_lr = 0.0010106125125756008\n",
    "lr_best_coarse = np.log10(best_coarse_lr)\n",
    "lr_min = lr_best_coarse - 0.1\n",
    "lr_max = lr_best_coarse + 0.1\n",
    "for i in range(10):\n",
    "    lr = lr_min + (lr_max - lr_min) * rng.random()\n",
    "    lr = 10**lr\n",
    "    train_params['learning_rate'] = lr\n",
    "    train_params['batch_size'] = random.choice(batch_choices)\n",
    "    train_params['hidden_size'] = random.choice(hidden_choices)\n",
    "    train_params['num_layers'] = random.choice(layers_choices)\n",
    "    \n",
    "    DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "    DP.load_data()\n",
    "    DP.preprocess()\n",
    "\n",
    "    X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "    Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "    X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "    Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "    # 70% train, 15% val, 15% test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "    X_val_t   = torch.from_numpy(X_val).float()\n",
    "    Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "    X_test_t  = torch.from_numpy(X_test).float()\n",
    "    Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "    val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "    test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    model = GRUModel(\n",
    "        input_size=DP.K,\n",
    "        hidden_size=train_params['hidden_size'],\n",
    "        num_layers=train_params['num_layers'],\n",
    "        output_size=DP.K,\n",
    "    ).to(device)\n",
    "    \n",
    "    best_model, train_loss, val_loss, val_loss_epoch, best_smooth_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)\n",
    "    print(f\"Model completed - lr={train_params['learning_rate']}, batch_size={train_params['batch_size']}, hidden_size={train_params['hidden_size']}, num_layers={train_params['num_layers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4968826",
   "metadata": {},
   "source": [
    "### Find single hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 50,\n",
    "    'hidden_size': 145,\n",
    "    'num_layers': 2,\n",
    "    'num_epochs': 5,\n",
    "    'learning_rate': 0.000845663544150917\n",
    "}\n",
    "\n",
    "# Single parameter to optimize\n",
    "param_list = [50, 55, 60, 65, 70]\n",
    "for parameter in param_list:\n",
    "    train_params['batch_size'] = parameter\n",
    "    \n",
    "    DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "    DP.load_data()\n",
    "    DP.preprocess()\n",
    "\n",
    "    X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "    Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "    X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "    Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "    # 70% train, 15% val, 15% test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "    X_val_t   = torch.from_numpy(X_val).float()\n",
    "    Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "    X_test_t  = torch.from_numpy(X_test).float()\n",
    "    Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "    val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "    test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    model = GRUModel(\n",
    "        input_size=DP.K,\n",
    "        hidden_size=train_params['hidden_size'],\n",
    "        num_layers=train_params['num_layers'],\n",
    "        output_size=DP.K,\n",
    "    ).to(device)\n",
    "    \n",
    "    best_model, train_loss, val_loss, val_loss_epoch, best_smooth_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)\n",
    "    print(f\"Model completed - lr={train_params['learning_rate']}, batch_size={train_params['batch_size']}, hidden_size={train_params['hidden_size']}, num_layers={train_params['num_layers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "loss_fig_name = f'loss_fig_loss_{best_smooth_loss}_epoch_{best_epoch}_update_step_{best_update_step}.png'\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Smooth Training Loss')\n",
    "plt.xlabel('update steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig(loss_fig_name, bbox_inches='tight', facecolor='none', pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss history\n",
    "val_loss_fig_name = f'val_loss_fig_loss_{best_smooth_loss}_epoch_{best_epoch}_update_step_{best_update_step}.png'\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(val_loss, label='Smooth Validation Loss', color='orange')\n",
    "plt.xlabel('update steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig(loss_fig_name, bbox_inches='tight', facecolor='none', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d0008",
   "metadata": {},
   "source": [
    "### Text synthesis and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a107588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sampling(prob, threshold = 0.9):\n",
    "    \"\"\"Set a threshold then filter the words by it's probability in text\n",
    "\n",
    "    Args: \n",
    "        prob: list of probability of words\n",
    "        threshold: threshold value\n",
    "    Return: \n",
    "        the index to sample\n",
    "    \"\"\"\n",
    "    # sort the probility\n",
    "    idx_prob = sorted(list(enumerate(prob)),key= lambda x:x[1],reverse= True)\n",
    "\n",
    "    # find the cut off point\n",
    "    cumulative = 0.0\n",
    "    cut_off_point = 0\n",
    "    for i,(_,p) in enumerate(idx_prob):\n",
    "        cumulative +=p\n",
    "        if cumulative >= threshold:\n",
    "            cut_off_point = i + 1\n",
    "            break\n",
    "\n",
    "\n",
    "    # get the probs before cut off point\n",
    "    candidate = idx_prob[:cut_off_point]\n",
    "    index, p = zip(*candidate)\n",
    "\n",
    "    # normalization\n",
    "    total = sum(p)\n",
    "    normal = (p_i/total for p_i in p)\n",
    "\n",
    "    # sampling\n",
    "    r = random.random()\n",
    "    cum = 0.0\n",
    "    for idx, p_i in zip(index, normal):\n",
    "        cum += p_i\n",
    "        if r < cum:\n",
    "            return idx\n",
    "\n",
    "def generate_text(model, hprev, DP, text_len, input_text, nucleus_sample=False, temperature=0.5):\n",
    "    \"\"\"Synthesises a string of given length based on previous text and temperature.\n",
    "        Based on this: https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb#scrollTo=y0rveBdAGeEz\n",
    "\n",
    "    Args:\n",
    "        model (nn.Model): Model from which to generate text\n",
    "        hprev (_type_): previous hidden states (1, 1, HIDDEN_SIZE)\n",
    "        DP (DataPreprocessing Class): data preprocessing\n",
    "        text_len (int): lentgh of text to generate\n",
    "        input_text (string): start string of synthesis\n",
    "        temperature (float, optional): temperature. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        string: Synthesized text\n",
    "    \"\"\"\n",
    "    \n",
    "    input_indicies = []\n",
    "    for char in input_text:\n",
    "        input_indicies.append(DP.get_one_hot_encoding(char))\n",
    "    input_indicies = np.concatenate(input_indicies, axis=1) # (K, SEQ_LENGTH)\n",
    "    input_indicies = input_indicies.T # (SEQ_LENGTH, K)\n",
    "    input_indicies = np.expand_dims(input_indicies, axis=0) # (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "    input_indicies = torch.tensor(input_indicies, dtype=torch.float, device=device)    \n",
    "    generated_text = input_text\n",
    "    \n",
    "    model.eval()\n",
    "    for _ in range(text_len):\n",
    "        predictions, hnext = model(input_indicies, hprev) # (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "        hprev = hnext\n",
    "        predictions = predictions[0, 0, :] # (K)\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predictions = torch.softmax(predictions, dim=-1)\n",
    "        prediction_id = None\n",
    "        if nucleus_sample:\n",
    "            prediction_id = nucleus_sampling(predictions)\n",
    "        else:\n",
    "            prediction_id = torch.multinomial(predictions, num_samples=1).item() # sample\n",
    "        next_char = DP.ind_to_char[prediction_id]\n",
    "        generated_text += next_char\n",
    "        \n",
    "        # Update the next input char\n",
    "        input_indicies = torch.zeros(1, 1, DP.K, device=device)\n",
    "        input_indicies[0, 0, prediction_id] = 1.0\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51114dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprev = model.init_hidden(num_layers=train_params['num_layers'], batch_size=1, hidden_size=train_params['hidden_size'])\n",
    "text = generate_text(model, hprev, DP, 1000, \"ROMEO: \", nucleus_sample=False, temperature=0.5)\n",
    "print(text)\n",
    "hprev = model.init_hidden(num_layers=train_params['num_layers'], batch_size=train_params['batch_size'], hidden_size=train_params['hidden_size'])\n",
    "\n",
    "mean_test_loss, test_losses = validation_loop(model, hprev, test_loader, device)\n",
    "print(f'\\nFinal mean test accuarcy = {mean_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969d5db",
   "metadata": {},
   "source": [
    "### Evaluation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Score\n",
    "# Convert the generated text into a list of equal length strings\n",
    "n = len(text)\n",
    "text_segments = [text[i:i+n] for i in range(0, len(text), n)]\n",
    "\n",
    "# Convert training data to text format\n",
    "train_dsl = []\n",
    "for x, _ in train_ds:  \n",
    "    text_sequence = DP.get_decoded_one_hot(x.numpy().T)\n",
    "    train_dsl.append(text_sequence[:n])\n",
    "\n",
    "# Make sure we have same number of candidates and references\n",
    "num_samples = min(len(text_segments), len(train_dsl))\n",
    "cands = text_segments[:num_samples]\n",
    "refs = train_dsl[:num_samples]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = bert_score_score(\n",
    "    cands=cands,\n",
    "    refs=refs,\n",
    "    lang=\"en\",\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    batch_size=train_params['batch_size'],\n",
    "    device=str(device),\n",
    "    rescale_with_baseline=True\n",
    ")\n",
    "\n",
    "print(f'Precision: {P.mean()}, Recall: {R.mean()}, F1: {F1.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=m.Spellpercentage()\n",
    "print(sp.compute_spellpercentage(text))\n",
    "Processor = DP\n",
    "perplexity = g.PerplexityGRU(\n",
    "    model=model,\n",
    "    processor=DP,\n",
    "    device=device\n",
    ")\n",
    "perplexity_score = perplexity.compute_perplexity(text)\n",
    "print(f\"Perplexity score: {perplexity_score}\")\n",
    "scorer = m.SelfBLEU(DP, max_n=4)\n",
    "print(\"Self-BLEU:\", scorer._self_bleu(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
