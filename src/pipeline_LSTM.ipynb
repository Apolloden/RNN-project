{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b11277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import itertools\n",
    "import metrics as m\n",
    "import perplexityLSTM as p\n",
    "from bert_score import score as bert_score_score\n",
    "from typing import List, Union, Optional, Tuple, Dict, Any\n",
    "import copy\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f'Device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50175e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed=61):\n",
    "    random.seed(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.manual_seed(seed) \n",
    "        except AttributeError:\n",
    "            pass \n",
    "\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n",
    "set_global_seed(61)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beff39a",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ba0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    def __init__(self, file_path, seq_length, batch_size):\n",
    "        self.file_path = file_path\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_seq = None\n",
    "        self.data = None\n",
    "        self.X_data = None\n",
    "        self.Y_data = None\n",
    "        self.K = None\n",
    "        self.char_to_ind = None\n",
    "        self.ind_to_char = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Prepares all the data necessary to train a model\n",
    "        \"\"\"\n",
    "        fid = open(self.file_path, \"r\")\n",
    "        book_data = fid.read()\n",
    "        fid.close()\n",
    "        self.data = book_data\n",
    "        unique_chars = list(set(book_data))\n",
    "        K = len(unique_chars)\n",
    "        self.K = K\n",
    "        mapping_value = np.arange(K)\n",
    "        char_to_ind = dict(zip(unique_chars, mapping_value))\n",
    "        ind_to_char = dict(zip(mapping_value, unique_chars))\n",
    "        self.char_to_ind = char_to_ind\n",
    "        self.ind_to_char = ind_to_char\n",
    "\n",
    "    def get_one_hot_encoding(self, X_chars):\n",
    "        \"\"\"Encodes text as a one hot array\n",
    "\n",
    "        Args:\n",
    "            char_to_ind (dict): the mapping\n",
    "            X_chars (string): characters to encode\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: one-hot encoding\n",
    "        \"\"\"\n",
    "        seq_length = len(X_chars)\n",
    "        one_hot = np.zeros((self.K, seq_length))\n",
    "        for i, char in enumerate(X_chars):\n",
    "            ind = self.char_to_ind[char]\n",
    "            one_hot[ind, i] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def get_decoded_one_hot(self, Y):\n",
    "        \"\"\"Decodes one-hot array back to text\n",
    "\n",
    "        Args:\n",
    "            ind_to_char (dict): the mapping\n",
    "            Y (np.ndarray): one-hot encoding\n",
    "\n",
    "        Returns:\n",
    "            string: the decoded text\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        for t in range(Y.shape[1]):\n",
    "            char_max = np.argmax(Y[:, t])\n",
    "            text += self.ind_to_char[char_max]\n",
    "        return text\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Prepares the data as a tuple of inputs and targets \n",
    "        \"\"\"\n",
    "        encoded_data = self.get_one_hot_encoding(self.data)\n",
    "        num_sequences = (len(self.data)-1) // self.seq_length # discarding the tail\n",
    "        self.num_seq = num_sequences\n",
    "        sequences_X = []\n",
    "        sequences_Y = []\n",
    "        t = 0 # pointer in text\n",
    "        for seq in range(num_sequences):\n",
    "            inputs = encoded_data[:, t: t+self.seq_length]\n",
    "            targets = encoded_data[:, t+1: t+self.seq_length+1]\n",
    "            sequences_X.append(inputs)\n",
    "            sequences_Y.append(targets)\n",
    "            t += self.seq_length\n",
    "        self.X_data = np.concatenate(sequences_X, axis=1)\n",
    "        self.Y_data = np.concatenate(sequences_Y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1c5f0",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://www.geeksforgeeks.org/long-short-term-memory-networks-using-pytorch/\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Creates a LSTM model with specified number of layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hprev, cprev):\n",
    "        \"\"\"Forward pass for the model\n",
    "        Args:\n",
    "            x (torch.Tensor): text (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "            hprev (torch.Tensor): previous hidden states (1, BATCH_SIZE, HIDDEN_STATES)\n",
    "            cprev (torch.Tensor): previous memory states (TODO)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor, torch.Tensor: logits, updated hidden states and memory states\n",
    "        \"\"\"\n",
    "        out, (hnext, cnext) = self.lstm(x, (hprev, cprev))\n",
    "        logits = self.fc(out)\n",
    "        return logits, hnext, cnext\n",
    "\n",
    "    def init_hidden_and_memory(self, num_layers, batch_size, hidden_size):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): number of layers\n",
    "            batch_size (int): size of batches\n",
    "            hidden_size (int): number of hidden neurons\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: initalized hidden and memory states (NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_size, device=device)\n",
    "        c0 = torch.zeros(num_layers, batch_size, hidden_size, device=device)\n",
    "        return h0, c0\n",
    "    \n",
    "    # dropout\n",
    "    # def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):\n",
    "    #     super().__init__()\n",
    "    #     self.hidden_size = hidden_size\n",
    "    #     self.num_layers = num_layers\n",
    "    #     self.lstm = nn.LSTM(\n",
    "    #         input_size, hidden_size, num_layers,\n",
    "    #         batch_first=True,\n",
    "    #         dropout=dropout if num_layers > 1 else 0.0  # no dropout on the last layer\n",
    "    #     )\n",
    "    #     self.fc  = nn.Linear(hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd67bb",
   "metadata": {},
   "source": [
    "### Training and Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc771969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, hprev, cprev, val_loader, device):\n",
    "    \"\"\"Uses a separate dataset to measure performace of the trained model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the trained model\n",
    "        hprev (torch.Tensor): previous hidden states\n",
    "        val_loader (dataloader): dataloader with the data\n",
    "        device (device): device for tensors\n",
    "\n",
    "    Returns:\n",
    "        mean loss, a list of all losses\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    smooth_loss = None\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets_indices = torch.argmax(targets, dim=2) # Shape (BATCH_SIZE, SEQ_LENGTH)\n",
    "\n",
    "            outputs, hnext, cnext = model(inputs, hprev, cprev)\n",
    "            hprev = hnext\n",
    "            cprev = cnext\n",
    "            preds = outputs.permute(0,2,1)      # shape (BATCH_SIZE, K, SEQ_LENGTH)\n",
    "            loss  = criterion(preds, targets_indices)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss.item()\n",
    "            else: \n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss.item()\n",
    "            val_loss.append(smooth_loss)\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / len(val_loader), val_loss\n",
    "\n",
    "def train(model, train_params, train_loader, val_loader):\n",
    "    \"\"\"Trains the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Model): model to train\n",
    "        train_params (dict): specific parameters used for training\n",
    "        train_loader (dataloader): dataloader with training data\n",
    "        val_loader (dataloader): dataloader with validation data\n",
    "\n",
    "    Returns:\n",
    "        Training statistics and losses\n",
    "    \"\"\"\n",
    "    \n",
    "    num_epochs = train_params['num_epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    hidden_size = train_params['hidden_size']\n",
    "    num_layers = train_params['num_layers']\n",
    "    learning_rate = train_params['learning_rate']\n",
    "    \n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    update_step = 0\n",
    "    best_update_step = 0\n",
    "    best_epoch = 0\n",
    "    smooth_loss = None\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_loss_epoch = []\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        hprev, cprev = model.init_hidden_and_memory(num_layers, batch_size, hidden_size)\n",
    "        loss = 0.0\n",
    "        \n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            targets_indices = torch.argmax(targets, dim=2) # Shape (BATCH_SIZE, SEQ_LENGTH)# WHY?\n",
    "            hprev = hprev.detach()\n",
    "            cprev = cprev.detach()\n",
    "                        \n",
    "            # FORWARD PASS\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hnext, cnext = model.forward(inputs, hprev, cprev) # outputs: shape (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "            hprev = hnext # update hidden states\n",
    "            cprev = cnext # update memory states\n",
    "            preds = outputs.permute(0,2,1) # shape (BATCH_SIZE, K, SEQ_LENGTH)\n",
    "            loss  = criterion(preds, targets_indices)\n",
    "            \n",
    "            # BACKWARD PASS\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters with ADAM\n",
    "            optimizer.step()\n",
    "                    \n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss.item()\n",
    "            else: \n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss.item()\n",
    "            train_loss.append(smooth_loss)\n",
    "            \n",
    "            update_step += 1\n",
    "            \n",
    "        # Validation loss\n",
    "        validation_loss_mean, val_losses = validation_loop(model, hprev, cprev, val_loader, device)\n",
    "        val_loss = val_loss + val_losses\n",
    "        val_loss_epoch.append(validation_loss_mean)\n",
    "        \n",
    "        # Save best model\n",
    "        if validation_loss_mean < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = validation_loss_mean\n",
    "            best_update_step = update_step\n",
    "            best_model = copy.deepcopy(model)\n",
    "                        \n",
    "    print(f'---Training complete---\\nBest model had validation smooth loss: {best_val_loss}, at epoch: {best_epoch}, best update step {best_update_step}')\n",
    "    return best_model, train_loss, val_loss, val_loss_epoch, best_val_loss, best_epoch, best_update_step\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a1108",
   "metadata": {},
   "source": [
    "### Hyperparameter search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed783ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(model, \n",
    "                          param_grid:dict, \n",
    "                          train_func,\n",
    "                          train_ds,\n",
    "                          val_ds,\n",
    "                          device,\n",
    "                          max_trials:int = None):\n",
    "    \"\"\"\"\n",
    "    Performs a ranodm or grid search over the hyperparameters\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        param_grid (dict): hyperparameters to search\n",
    "        train_func (function): function to train the model\n",
    "        train_ds (TensorDataset): training dataset\n",
    "        val_ds (TensorDataset): validation dataset\n",
    "        device (device): device for tensors\n",
    "        max_trials (int, optional): maximum number of trials. Defaults to None.\n",
    "    Returns:\n",
    "        all parameter combinations and print best combination with lowest validation loss\n",
    "    \"\"\" \n",
    " \n",
    "    results = []\n",
    "    keys = list(param_grid.keys())\n",
    "    all_combinations = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "    if max_trials:\n",
    "    \n",
    "        all_combinations = random.sample(all_combinations, min(max_trials,len(all_combinations)))\n",
    "        \n",
    "\n",
    "    for trial_id, values in enumerate(all_combinations):\n",
    "        # id from0\n",
    "        trial_config = dict(zip(keys, values))\n",
    "        \n",
    "        print(f\"\\n Trial {trial_id + 1}/{len(all_combinations)}: {trial_config}\")\n",
    "\n",
    "        # create model\n",
    "        #model_train = model(**trial_config).to(device)\n",
    "        model_train = model(input_size=DP.K,\n",
    "               hidden_size=trial_config[\"hidden_size\"],\n",
    "                num_layers=trial_config[\"num_layers\"],\n",
    "               output_size=DP.K).to(device)\n",
    "\n",
    "        # train model\n",
    "        batch_size = trial_config['batch_size']\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        train_losses ,val_loss,val_loss_epoch, *_ = train_func(model_train, trial_config, train_loader, val_loader)\n",
    "\n",
    "\n",
    "        # record\n",
    "        trial_result = trial_config.copy()\n",
    "        trial_result['val_loss'] = round(min(val_loss_epoch), 4)\n",
    "        results.append(trial_result)\n",
    "        #print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # plot train loss\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(train_losses)\n",
    "        plt.title(f\"Training Loss for Trial {trial_id + 1}\")\n",
    "        plt.xlabel(\"Update steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.savefig(f\"train_loss_trial_{trial_id + 1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # plot val loss\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(val_loss, color='orange')\n",
    "        plt.title(f\"Validation Loss for Trial {trial_id + 1}\")\n",
    "        plt.xlabel(\"Update steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.savefig(f\"val_loss_trial_{trial_id + 1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # best result\n",
    "        best_result = min(results, key=lambda x: x['val_loss'])\n",
    "        print(f\"\\n Best Result so far: {best_result}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def ablation_experiments(model,\n",
    "                          param_name,\n",
    "                            param_values,\n",
    "                              fixed_config, \n",
    "                              train_func, \n",
    "                              train_ds,\n",
    "                                val_ds,\n",
    "                                  device):\n",
    "    \"\"\"\n",
    "    Performs ablation experiments by varying one parameter at a time\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        param_name (str): name of the hyperparameter to vary\n",
    "        param_values (list): values of the hyperparameter to test\n",
    "        fixed_config (dict): fixed hyperparameters\n",
    "        train_func (function): function to train the model\n",
    "        train_ds (TensorDataset): training dataset\n",
    "        val_ds (TensorDataset): validation dataset\n",
    "        device (device): device for tensors         \n",
    "        Returns:\n",
    "            results (list): list of results for each parameter value\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for val in param_values:\n",
    "        config = fixed_config.copy()\n",
    "        config[param_name] = val\n",
    "\n",
    "        model_train = model(\n",
    "            input_size=DP.K,\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            output_size=DP.K,\n",
    "            num_layers=1  \n",
    "        ).to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "        train_losses ,val_loss,val_loss_epoch, *_ = train_func(model_train, config, train_loader, val_loader)\n",
    "        print(val_loss_epoch)\n",
    "\n",
    "        # plot train loss\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(train_losses)\n",
    "        plt.title(f\"Training Loss \")\n",
    "        plt.xlabel(\"Update steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.savefig(f\"train_loss\")\n",
    "        plt.close()\n",
    "\n",
    "         # plot val loss\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(val_loss, color='orange')\n",
    "        plt.title(f\"Validation Loss \")\n",
    "        plt.xlabel(\"Update steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.savefig(f\"val_loss\")\n",
    "        plt.close()\n",
    "\n",
    "        results.append({\n",
    "            \"param_value\": val,\n",
    "            \"val_loss\": round(min(val_loss_epoch), 4)\n",
    "        })\n",
    "    print(results)   \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee00673",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3362229",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 75,\n",
    "    'num_layers': 4,\n",
    "    'num_epochs': 20,\n",
    "    'learning_rate': 0.004262771798835783\n",
    "}\n",
    "\n",
    "DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "DP.load_data()\n",
    "DP.preprocess()\n",
    "print(len(DP.data))\n",
    "\n",
    "X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "# 70% train, 15% val, 15% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "X_val_t   = torch.from_numpy(X_val).float()\n",
    "Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "X_test_t  = torch.from_numpy(X_test).float()\n",
    "Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "model = LSTMModel(\n",
    "    input_size=DP.K,\n",
    "    hidden_size=train_params['hidden_size'],\n",
    "    num_layers=train_params['num_layers'],\n",
    "    output_size=DP.K,\n",
    ").to(device)\n",
    "\n",
    "best_model, train_loss, val_loss, val_loss_epoch, best_val_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928aed5",
   "metadata": {},
   "source": [
    "### Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21884541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "loss_fig_name = f'loss_fig_loss_{best_val_loss}_epoch_{best_epoch}_update_step_{best_update_step}.png'\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Smooth Training Loss')\n",
    "plt.xlabel('update steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig(loss_fig_name, bbox_inches='tight', facecolor='none', pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss history\n",
    "val_loss_fig_name = f'val_loss_fig_loss_{best_val_loss}_epoch_{best_epoch}_update_step_{best_update_step}.png'\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(val_loss, label='Smooth Validation Loss', color='orange')\n",
    "plt.xlabel('update steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig(loss_fig_name, bbox_inches='tight', facecolor='none', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63b487",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a28e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {       \n",
    "#     \"hidden_size\": [64, 128, 256,512], \n",
    "#     \"learning_rate\": [0.0005, 0.001, 0.003],\n",
    "#     \"batch_size\": [8, 16, 32],\n",
    "#     \"num_epochs\": [20, 25,30],\n",
    "#     \"num_layers\": [1] # change to 2 for 2layer LSTM\n",
    "# }\n",
    "\n",
    "# results = hyperparameter_search(\n",
    "#     model=LSTMModel,\n",
    "#     param_grid=param_grid,\n",
    "#     train_func=train,\n",
    "#     train_ds=train_ds,\n",
    "#     val_ds=val_ds,\n",
    "#     device=device,\n",
    "#     max_trials=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02daf4b",
   "metadata": {},
   "source": [
    "### Ablation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Layer LSTM Optimal Parameters Choose\n",
    "# fixed_config = {\n",
    "#     \"hidden_size\": 256,\n",
    "#     \"learning_rate\": 0.001,\n",
    "#     \"batch_size\": 16,\n",
    "#     \"num_epochs\": 20,\n",
    "#     \"num_layers\":1\n",
    "# }\n",
    "# hidden_size_results = ablation_experiments(\n",
    "#     model=LSTMModel,\n",
    "#     param_name=\"num_epochs\",\n",
    "#     param_values=[15],\n",
    "#     fixed_config=fixed_config,\n",
    "#     train_func=train,\n",
    "#     train_ds=train_ds,\n",
    "#     val_ds=val_ds,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# Best parameters\n",
    "# fixed_config = {\n",
    "#     \"hidden_size\": 256,\n",
    "#     \"learning_rate\": 0.001,\n",
    "#     \"batch_size\": 16,\n",
    "#     \"num_epochs\": 15,\n",
    "#     \"num_layers\":1\n",
    "# }\n",
    "# Best val_loss around 1.66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d0008",
   "metadata": {},
   "source": [
    "### Text synthesis and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a107588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sampling(prob, threshold = 0.9):\n",
    "    \"\"\"Set a threshold then filter the words by it's probability in text\n",
    "\n",
    "    Args: \n",
    "        prob: list of probability of words\n",
    "        threshold: threshold value\n",
    "    Return: \n",
    "        the index to sample\n",
    "    \"\"\"\n",
    "    # sort the probility\n",
    "    idx_prob = sorted(list(enumerate(prob)),key= lambda x:x[1],reverse= True)\n",
    "\n",
    "    # find the cut off point\n",
    "    cumulative = 0.0\n",
    "    cut_off_point = 0\n",
    "    for i,(_,p) in enumerate(idx_prob):\n",
    "        cumulative +=p\n",
    "        if cumulative >= threshold:\n",
    "            cut_off_point = i + 1\n",
    "            break\n",
    "\n",
    "\n",
    "    # get the probs before cut off point\n",
    "    candidate = idx_prob[:cut_off_point]\n",
    "    index, p = zip(*candidate)\n",
    "\n",
    "    # normalization\n",
    "    total = sum(p)\n",
    "    normal = (p_i/total for p_i in p)\n",
    "\n",
    "    # sampling\n",
    "    r = random.random()\n",
    "    cum = 0.0\n",
    "    for idx, p_i in zip(index, normal):\n",
    "        cum += p_i\n",
    "        if r < cum:\n",
    "            return idx\n",
    "\n",
    "def generate_text(model, hprev, DP, text_len, input_text, nucleus_sample=False, temperature=0.5):\n",
    "    \"\"\"Synthesises a string of given length based on previous text and temperature.\n",
    "        Based on this: https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb#scrollTo=y0rveBdAGeEz\n",
    "\n",
    "    Args:\n",
    "        model (nn.Model): Model from which to generate text\n",
    "        hprev (_type_): previous hidden states (1, 1, HIDDEN_SIZE)\n",
    "        DP (DataPreprocessing Class): data preprocessing\n",
    "        text_len (int): lentgh of text to generate\n",
    "        input_text (string): start string of synthesis\n",
    "        temperature (float, optional): temperature. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        string: Synthesized text\n",
    "    \"\"\"\n",
    "    \n",
    "    input_indicies = []\n",
    "    for char in input_text:\n",
    "        input_indicies.append(DP.get_one_hot_encoding(char))\n",
    "    input_indicies = np.concatenate(input_indicies, axis=1) # (K, SEQ_LENGTH)\n",
    "    input_indicies = input_indicies.T # (SEQ_LENGTH, K)\n",
    "    input_indicies = np.expand_dims(input_indicies, axis=0) # (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "    input_indicies = torch.tensor(input_indicies, dtype=torch.float, device=device)    \n",
    "    generated_text = input_text\n",
    "    \n",
    "    model.eval()\n",
    "    for _ in range(text_len):\n",
    "        predictions, hnext = model(input_indicies, hprev) # (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "        hprev = hnext\n",
    "        predictions = predictions[0, 0, :] # (K)\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predictions = torch.softmax(predictions, dim=-1)\n",
    "        prediction_id = None\n",
    "        if nucleus_sample:\n",
    "            prediction_id = nucleus_sampling(predictions)\n",
    "        else:\n",
    "            prediction_id = torch.multinomial(predictions, num_samples=1).item() # sample\n",
    "        next_char = DP.ind_to_char[prediction_id]\n",
    "        generated_text += next_char\n",
    "        \n",
    "        # Update the next input char\n",
    "        input_indicies = torch.zeros(1, 1, DP.K, device=device)\n",
    "        input_indicies[0, 0, prediction_id] = 1.0\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51114dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprev, cprev = model.init_hidden_and_memory(num_layers=train_params['num_layers'], batch_size=1, hidden_size=train_params['hidden_size'])\n",
    "text = generate_text(model, hprev, cprev, DP, 1000, \"Romeo: \", nucleus_sample=False, temperature=0.5)\n",
    "print(text)\n",
    "hprev, cprev = model.init_hidden_and_memory(num_layers=train_params['num_layers'], batch_size=train_params['batch_size'], hidden_size=train_params['hidden_size'])\n",
    "\n",
    "mean_test_loss, test_losses = validation_loop(model, hprev, cprev, test_loader, device)\n",
    "print(f'\\nFinal mean test accuarcy = {mean_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2880c",
   "metadata": {},
   "source": [
    "### Evaluation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Score\n",
    "# Convert the generated text into a list of equal length strings\n",
    "n = len(text)\n",
    "text_segments = [text[i:i+n] for i in range(0, len(text), n)]\n",
    "\n",
    "# Convert training data to text format\n",
    "train_dsl = []\n",
    "for x, _ in train_ds:  \n",
    "    text_sequence = DP.get_decoded_one_hot(x.numpy().T)\n",
    "    train_dsl.append(text_sequence[:n])\n",
    "\n",
    "# Make sure we have same number of candidates and references\n",
    "num_samples = min(len(text_segments), len(train_dsl))\n",
    "cands = text_segments[:num_samples]\n",
    "refs = train_dsl[:num_samples]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = bert_score_score(\n",
    "    cands=cands,\n",
    "    refs=refs,\n",
    "    lang=\"en\",\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    batch_size=train_params['batch_size'],\n",
    "    device=str(device),\n",
    "    rescale_with_baseline=True\n",
    ")\n",
    "\n",
    "print(f'Precision: {P.mean()}, Recall: {R.mean()}, F1: {F1.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=m.Spellpercentage()\n",
    "print(sp.compute_spellpercentage(text))\n",
    "Processor = DP\n",
    "perplexity = p.PerplexityLSTM(\n",
    "    model=model,\n",
    "    processor=DP,\n",
    "    device=device\n",
    ")\n",
    "perplexity_score = perplexity.compute_perplexity(text)\n",
    "print(f\"Perplexity score: {perplexity_score}\")\n",
    "scorer = m.SelfBLEU(DP, max_n=4)\n",
    "print(\"Self-BLEU:\", scorer._self_bleu(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
