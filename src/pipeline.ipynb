{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b11277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "rng = np.random.default_rng()\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f'Device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beff39a",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ba0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    def __init__(self, file_path, seq_length, batch_size):\n",
    "        self.file_path = file_path\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_seq = None\n",
    "        self.data = None\n",
    "        self.X_data = None\n",
    "        self.Y_data = None\n",
    "        self.K = None\n",
    "        self.char_to_ind = None\n",
    "        self.ind_to_char = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Prepares all the data necessary to train a model\n",
    "        \"\"\"\n",
    "        fid = open(self.file_path, \"r\")\n",
    "        book_data = fid.read()\n",
    "        fid.close()\n",
    "        self.data = book_data\n",
    "        unique_chars = list(set(book_data))\n",
    "        K = len(unique_chars)\n",
    "        self.K = K\n",
    "        mapping_value = np.arange(K)\n",
    "        char_to_ind = dict(zip(unique_chars, mapping_value))\n",
    "        ind_to_char = dict(zip(mapping_value, unique_chars))\n",
    "        self.char_to_ind = char_to_ind\n",
    "        self.ind_to_char = ind_to_char\n",
    "\n",
    "    def get_one_hot_encoding(self, X_chars):\n",
    "        \"\"\"Encodes text as a one hot array\n",
    "\n",
    "        Args:\n",
    "            char_to_ind (dict): the mapping\n",
    "            X_chars (string): characters to encode\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: one-hot encoding\n",
    "        \"\"\"\n",
    "        seq_length = len(X_chars)\n",
    "        one_hot = np.zeros((self.K, seq_length))\n",
    "        for i, char in enumerate(X_chars):\n",
    "            ind = self.char_to_ind[char]\n",
    "            one_hot[ind, i] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def get_decoded_one_hot(self, Y):\n",
    "        \"\"\"Decodes one-hot array back to text\n",
    "\n",
    "        Args:\n",
    "            ind_to_char (dict): the mapping\n",
    "            Y (np.ndarray): one-hot encoding\n",
    "\n",
    "        Returns:\n",
    "            string: the decoded text\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        for t in range(Y.shape[1]):\n",
    "            char_max = np.argmax(Y[:, t])\n",
    "            text += self.ind_to_char[char_max]\n",
    "        return text\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Prepares the data as a tuple of inputs and targets \n",
    "        \"\"\"\n",
    "        encoded_data = self.get_one_hot_encoding(self.data)\n",
    "        num_sequences = (len(self.data)-1) // self.seq_length # discarding the tail\n",
    "        self.num_seq = num_sequences\n",
    "        sequences_X = []\n",
    "        sequences_Y = []\n",
    "        t = 0 # pointer in text\n",
    "        for seq in range(num_sequences):\n",
    "            inputs = encoded_data[:, t: t+self.seq_length]\n",
    "            targets = encoded_data[:, t+1: t+self.seq_length+1]\n",
    "            sequences_X.append(inputs)\n",
    "            sequences_Y.append(targets)\n",
    "            t += self.seq_length\n",
    "        self.X_data = np.concatenate(sequences_X, axis=1)\n",
    "        self.Y_data = np.concatenate(sequences_Y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1c5f0",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Creates a 1 layer RNN model, first there is a recurrent layer, \n",
    "    then a fully connected output layer. The activation function used is tanh.\n",
    "\n",
    "    Args:\n",
    "        nn (RNN): RNN model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, hprev):\n",
    "        \"\"\"Forward pass for the model\n",
    "        Args:\n",
    "            x (torch.Tensor): text (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "            hprev (torch.Tensor): previous hidden states (1, BATCH_SIZE, HIDDEN_STATES)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor, torch.Tensor: logits and updated hidden states\n",
    "        \"\"\"\n",
    "        out, hnext = self.rnn(x, hprev)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hnext\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        \"\"\"Initializes a new hidden layers of zeroes\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): batches\n",
    "            hidden_size (int): neurons in the hidden layer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: a new hidden layer (1, BATCH_SIZE, HIDDEN_SIZE)\n",
    "        \"\"\"\n",
    "        return torch.zeros(1, batch_size, hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd67bb",
   "metadata": {},
   "source": [
    "### Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc771969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, hprev, val_loader, device):\n",
    "    \"\"\"Uses a separate dataset to measure performace of the trained model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the trained model\n",
    "        hprev (torch.Tensor): previous hidden states\n",
    "        val_loader (dataloader): dataloader with the data\n",
    "        device (device): device for tensors\n",
    "\n",
    "    Returns:\n",
    "        mean loss, a list of all losses\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    smooth_loss = None\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets_indices = torch.argmax(targets, dim=2) # Shape (BATCH_SIZE, SEQ_LENGTH)\n",
    "\n",
    "            outputs, hnext = model(inputs, hprev)\n",
    "            hprev = hnext\n",
    "            preds = outputs.permute(0,2,1)      # shape (BATCH_SIZE, K, SEQ_LENGTH)\n",
    "            loss  = criterion(preds, targets_indices)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss.item()\n",
    "            else: \n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss.item()\n",
    "            val_loss.append(smooth_loss)\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / len(val_loader), val_loss\n",
    "\n",
    "def train(model, train_params, train_loader, val_loader):\n",
    "    \"\"\"Trains the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Model): model to train\n",
    "        train_params (dict): specific parameters used for training\n",
    "        train_loader (dataloader): dataloader with training data\n",
    "        val_loader (dataloader): dataloader with validation data\n",
    "\n",
    "    Returns:\n",
    "        Training statistics and losses\n",
    "    \"\"\"\n",
    "    \n",
    "    num_epochs = train_params['num_epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    hidden_size = train_params['hidden_size']\n",
    "    learning_rate = train_params['learning_rate']\n",
    "    \n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    update_step = 0\n",
    "    best_update_step = 0\n",
    "    best_epoch = 0\n",
    "    smooth_loss = None\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_loss_epoch = []\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        hprev = model.init_hidden(batch_size, hidden_size)\n",
    "        loss = 0.0\n",
    "        \n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            targets_indices = torch.argmax(targets, dim=2) # Shape (BATCH_SIZE, SEQ_LENGTH)\n",
    "            hprev = hprev.detach()\n",
    "                        \n",
    "            # FORWARD PASS\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hnext = model.forward(inputs, hprev) # outputs: shape (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "            hprev = hnext # update hidden states\n",
    "            preds = outputs.permute(0,2,1) # shape (BATCH_SIZE, K, SEQ_LENGTH)\n",
    "            loss  = criterion(preds, targets_indices)\n",
    "            \n",
    "            # BACKWARD PASS\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters with ADAM\n",
    "            optimizer.step()\n",
    "                    \n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss.item()\n",
    "            else: \n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss.item()\n",
    "            train_loss.append(smooth_loss)\n",
    "            \n",
    "            update_step += 1\n",
    "            \n",
    "        # Validation loss\n",
    "        validation_loss_mean, val_losses = validation_loop(model, hprev, val_loader, device)\n",
    "        val_loss = val_loss + val_losses\n",
    "        val_loss_epoch.append(validation_loss_mean)\n",
    "        \n",
    "         # Save best model\n",
    "        if validation_loss_mean < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = validation_loss_mean\n",
    "            best_update_step = update_step\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "    print(f'---Training complete---\\nBest model had validation smooth loss: {best_val_loss}, at epoch: {best_epoch}, best update step {best_update_step}')\n",
    "    return best_model, train_loss, val_loss, val_loss_epoch, best_val_loss, best_epoch, best_update_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee00673",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3362229",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "DP.load_data()\n",
    "DP.preprocess()\n",
    "\n",
    "X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "# 70% train, 15% val, 15% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "X_val_t   = torch.from_numpy(X_val).float()\n",
    "Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "X_test_t  = torch.from_numpy(X_test).float()\n",
    "Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=train_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "model = RNNModel(\n",
    "    input_size=DP.K,\n",
    "    hidden_size=train_params['hidden_size'],\n",
    "    num_layers=train_params['num_layers'],\n",
    "    output_size=DP.K,\n",
    ").to(device)\n",
    "\n",
    "best_model, train_loss, val_loss, val_loss_epoch, best_smooth_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3b50e",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd79365",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'num_epochs': 30,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "# Coarse random search for learning rate parameter\n",
    "batch_choices = [64, 128]\n",
    "hidden_choices = [150, 250, 500]\n",
    "lr_min = -4\n",
    "lr_max = -2\n",
    "for i in range(10):\n",
    "    lr = lr_min + (lr_max - lr_min) * rng.random()\n",
    "    lr = 10**lr\n",
    "    train_params['learning_rate'] = lr\n",
    "    train_params['batch_size'] = random.choice(batch_choices)\n",
    "    train_params['hidden_size'] = random.choice(hidden_choices)\n",
    "    \n",
    "    DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "    DP.load_data()\n",
    "    DP.preprocess()\n",
    "\n",
    "    X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "    Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "    X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "    Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "    # 70% train, 15% val, 15% test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "    X_val_t   = torch.from_numpy(X_val).float()\n",
    "    Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "    X_test_t  = torch.from_numpy(X_test).float()\n",
    "    Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "    val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "    test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    model = RNNModel(\n",
    "        input_size=DP.K,\n",
    "        hidden_size=train_params['hidden_size'],\n",
    "        num_layers=train_params['num_layers'],\n",
    "        output_size=DP.K,\n",
    "    ).to(device)\n",
    "    \n",
    "    \n",
    "    best_model, train_loss, val_loss, val_loss_epoch, best_smooth_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)\n",
    "    print(f\"Model completed - lr={train_params['learning_rate']}, batch_size={train_params['batch_size']}, hidden_size={train_params['hidden_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd26a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'file_path': './shakes.txt',\n",
    "    'seq_length': 25,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'num_epochs': 20,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "# Fine random search for lr parameter\n",
    "batch_choices = [64]\n",
    "hidden_choices = [150]\n",
    "best_coarse_lr = 0.003059284171933464\n",
    "lr_best_coarse = np.log10(best_coarse_lr)\n",
    "lr_min = lr_best_coarse - 0.1\n",
    "lr_max = lr_best_coarse + 0.1\n",
    "for i in range(10):\n",
    "    lr = lr_min + (lr_max - lr_min) * rng.random()\n",
    "    lr = 10**lr\n",
    "    train_params['learning_rate'] = lr\n",
    "    train_params['batch_size'] = random.choice(batch_choices)\n",
    "    train_params['hidden_size'] = random.choice(hidden_choices)\n",
    "    \n",
    "    DP = DataPreprocessing(train_params['file_path'], train_params['seq_length'], train_params['batch_size'])\n",
    "    DP.load_data()\n",
    "    DP.preprocess()\n",
    "\n",
    "    X_data = DP.X_data.T # shape (num_seq * seq_length, K)\n",
    "    Y_data = DP.Y_data.T # shape (num_seq * seq_length, K)\n",
    "    X_data = X_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "    Y_data = Y_data.reshape(DP.num_seq, train_params['seq_length'], DP.K) # shape (num_seq, seq_length, K)\n",
    "\n",
    "    # 70% train, 15% val, 15% test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_data, Y_data, test_size=0.15, shuffle=False)\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_train, Y_train, test_size=0.176, shuffle=False)\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    Y_train_t = torch.from_numpy(Y_train).float()\n",
    "\n",
    "    X_val_t   = torch.from_numpy(X_val).float()\n",
    "    Y_val_t   = torch.from_numpy(Y_val).float()\n",
    "\n",
    "    X_test_t  = torch.from_numpy(X_test).float()\n",
    "    Y_test_t  = torch.from_numpy(Y_test).float()\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "    val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "    test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=train_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    model = RNNModel(\n",
    "        input_size=DP.K,\n",
    "        hidden_size=train_params['hidden_size'],\n",
    "        num_layers=train_params['num_layers'],\n",
    "        output_size=DP.K,\n",
    "    ).to(device)\n",
    "    \n",
    "    \n",
    "    best_model, train_loss, val_loss, val_loss_epoch, best_smooth_loss, best_epoch, best_update_step = train(model, train_params, train_loader, val_loader)\n",
    "    print(f\"Model completed - lr={train_params['learning_rate']}, batch_size={train_params['batch_size']}, hidden_size={train_params['hidden_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219e56e",
   "metadata": {},
   "source": [
    "### Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "loss_fig_name = f'loss_fig_loss_{best_smooth_loss}_epoch_{best_epoch}_update_step_{best_update_step}.png'\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Smooth Training Loss')\n",
    "plt.xlabel('update steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig(loss_fig_name, bbox_inches='tight', facecolor='none', pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss history\n",
    "print(val_loss[-1])\n",
    "val_loss_fig_name = f'val_loss_fig_loss_{best_smooth_loss}_epoch_{best_epoch}_update_step_{best_update_step}.png'\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(val_loss, label='Smooth Validation Loss', color='orange')\n",
    "plt.xlabel('update steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig(loss_fig_name, bbox_inches='tight', facecolor='none', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d0008",
   "metadata": {},
   "source": [
    "### Text synthesis and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a107588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sampling(prob, threshold = 0.9):\n",
    "    \"\"\"Set a threshold then filter the words by it's probability in text\n",
    "\n",
    "    Args: \n",
    "        prob: list of probability of words\n",
    "        threshold: threshold value\n",
    "    Return: \n",
    "        the index to sample\n",
    "    \"\"\"\n",
    "    # 1 sorted the probility\n",
    "    idx_prob = sorted(list(enumerate(prob)),key= lambda x:x[1],reverse= True)\n",
    "\n",
    "    # 2 find the cut off point\n",
    "    cumulative = 0.0\n",
    "    cut_off_point = 0\n",
    "    for i,(_,p) in enumerate(idx_prob):\n",
    "        cumulative +=p\n",
    "        if cumulative >= threshold:\n",
    "            cut_off_point = i + 1\n",
    "            break\n",
    "\n",
    "\n",
    "    # 3 get the probs before cut off point\n",
    "    candidate = idx_prob[:cut_off_point]\n",
    "    index, p = zip(*candidate)\n",
    "\n",
    "    # 4 normalization\n",
    "    total = sum(p)\n",
    "    normal = (p_i/total for p_i in p)\n",
    "\n",
    "    # 5 sampling\n",
    "    r = random.random()\n",
    "    cum = 0.0\n",
    "    for idx, p_i in zip(index, normal):\n",
    "        cum += p_i\n",
    "        if r < cum:\n",
    "            return idx\n",
    "\n",
    "def generate_text(model, hprev, DP, text_len, input_text, nucleus_sample=False, temperature=0.5):\n",
    "    \"\"\"Synthesises a string of given length based on previous text and temperature.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Model): Model from which to generate text\n",
    "        hprev (_type_): previous hidden states (1, 1, HIDDEN_SIZE)\n",
    "        DP (DataPreprocessing Class): data preprocessing\n",
    "        text_len (int): lentgh of text to generate\n",
    "        input_text (string): start string of synthesis\n",
    "        temperature (float, optional): temperature. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        string: Synthesized text\n",
    "    \"\"\"\n",
    "    \n",
    "    input_indicies = []\n",
    "    for char in input_text:\n",
    "        input_indicies.append(DP.get_one_hot_encoding(char))\n",
    "    input_indicies = np.concatenate(input_indicies, axis=1) # (K, SEQ_LENGTH)\n",
    "    input_indicies = input_indicies.T # (SEQ_LENGTH, K)\n",
    "    input_indicies = np.expand_dims(input_indicies, axis=0) # (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "    input_indicies = torch.tensor(input_indicies, dtype=torch.float, device=device)    \n",
    "    generated_text = input_text\n",
    "    \n",
    "    model.eval()\n",
    "    for _ in range(text_len):\n",
    "        predictions, hnext = model(input_indicies, hprev) # (BATCH_SIZE, SEQ_LENGTH, K)\n",
    "        hprev = hnext\n",
    "        predictions = predictions[0, 0, :] # (K)\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predictions = torch.softmax(predictions, dim=-1)\n",
    "        prediction_id = None\n",
    "        if nucleus_sample:\n",
    "            prediction_id = nucleus_sampling(predictions)\n",
    "        else:\n",
    "            prediction_id = torch.multinomial(predictions, num_samples=1).item() # sample\n",
    "        next_char = DP.ind_to_char[prediction_id]\n",
    "        generated_text += next_char\n",
    "        \n",
    "        # Update the next input char\n",
    "        input_indicies = torch.zeros(1, 1, DP.K, device=device)\n",
    "        input_indicies[0, 0, prediction_id] = 1.0\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51114dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprev = model.init_hidden(batch_size=1, hidden_size=train_params['hidden_size'])\n",
    "text = generate_text(model, hprev, DP, 200, \"ROMEO: \", nucleus_sample=False, temperature=0.5)\n",
    "print(text)\n",
    "hprev = model.init_hidden(batch_size=train_params['batch_size'], hidden_size=train_params['hidden_size'])\n",
    "\n",
    "mean_test_loss, test_losses = validation_loop(model, hprev, test_loader, device)\n",
    "print(f'\\nFinal mean test accuarcy = {mean_test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
